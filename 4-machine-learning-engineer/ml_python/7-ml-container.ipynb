{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990218d3-b1bf-4fd7-8359-44a8040c6379",
   "metadata": {},
   "source": [
    "# Machine Learning with Python - Containerizing a Model\n",
    "Moving to production\n",
    "\n",
    "### docker, flask, and sklearn\n",
    "Provides lots of tools to help!\n",
    "\n",
    "![](app/docker-flask.jpg)\n",
    "\n",
    "* docker packages everything up as a **microservice**\n",
    "* flask is a simple python **webserver** so we can incorporte our python objects easily\n",
    "* scikit-learn models can be saved or **persisted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991d3b7-e7a7-4235-81f7-8dfee6a22603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier                              \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2958cf-694e-4d62-b9ec-020b2cbf70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in our titanic data\n",
    "df_og = pd.read_csv('data/train.csv') \n",
    "\n",
    "#split the data set into train and test sets remove any non-numeric columns for the example\n",
    "X, y = df_og.drop(columns=['PassengerId','Name','Ticket','Cabin','Embarked','Survived']), df_og['Survived']\n",
    "X = X.replace({'male': 0, 'female': 1}).fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print('size of X_train') \n",
    "print(X_train.shape)\n",
    "print('size of X_test')\n",
    "print(X_test.shape)\n",
    "print('size of y_train') \n",
    "print(y_train.shape)\n",
    "print('size of y_test')\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9eef4-4956-4073-b4c8-dab9e68f7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_train)\n",
    "\n",
    "print('train acc:', accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print('testa acc:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf4589-5701-4643-8964-6d2e27bc26eb",
   "metadata": {},
   "source": [
    "## Persist the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1e6da-ac0e-491c-912f-8cd2418eeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(rf, 'app/model.pkl') #pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52551c7b-2468-47ee-ae07-8d8cab390363",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca2bdd-b44e-43be-9091-35b12e282ede",
   "metadata": {},
   "source": [
    "Need to do a few things now. Following some guidance from this old [blog post](https://towardsdatascience.com/a-flask-api-for-serving-scikit-learn-models-c8bcdaa41daa) and using chatGPT.\n",
    "\n",
    "* create a flask app\n",
    "* import this model\n",
    "* create and endpoint or **route** as an API (application progamming interface) to pass the data\n",
    "* package everything up into a docker container using **`docker build`** and testing with **`docker run`**\n",
    "\n",
    "There are some important considerations for stability of this program that we haven't discussed in detail but are important:\n",
    "\n",
    "* inputs to the API need to be clean and formated\n",
    "* The proccess of preparing data is called a **data pipeline**\n",
    "* API should have a **contract** with the rest of the microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59270e-fa1a-4f30-9213-ff4e6c7f8337",
   "metadata": {},
   "source": [
    "### Testing our API\n",
    "we can use **curl** to test the API locally.\n",
    "\n",
    "`curl -X POST 127.0.0.1:8080/predict -H 'Content-Type: application/json' -d '[{\"f1\":0.80576177,\"f2\":1.37593746,\"f3\":-0.09609774,\"f4\":-0.46983664,\"f5\":-0.46399264,\"f6\":-0.41596074}]'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889b258-033a-4892-8935-cb004838cea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
